{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Two-Stage Funnel Selection\n",
    "\n",
    "This notebook performs **two-stage feature selection** to reduce high-dimensional genomic data to a manageable size for quantum machine learning:\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Stage 1 - Mutual Information Filter**: Coarse sieve using univariate MI scores (50K features)\n",
    "2. **Stage 2 - XGBoost Importance**: Fine-toothed comb using tree-based feature importance (500 features)\n",
    "3. **NaN Validation**: Verify output data quality\n",
    "4. **Packaging**: Copy indicators and create archive\n",
    "\n",
    "## Input/Output:\n",
    "- **Input**: `final_filtered_datasets/` (imputed modality parquets + indicator_features.parquet)\n",
    "- **Output**: `final_processed_datasets_xgb_balanced/` (6 modality parquets @ 500 features each + indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T09:07:32.406100Z",
     "iopub.status.busy": "2025-09-22T09:07:32.405723Z",
     "iopub.status.idle": "2025-09-22T09:59:37.779679Z",
     "shell.execute_reply": "2025-09-22T09:59:37.778375Z",
     "shell.execute_reply.started": "2025-09-22T09:07:32.406071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Adjust paths and parameters here\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # Paths (adjust for your environment)\n",
    "    'source_dir': '/kaggle/input/data-process/quantum-classification/final_filtered_datasets',\n",
    "    'output_dir': 'final_processed_datasets_xgb_balanced',\n",
    "    'indicator_file': '/kaggle/input/data-process/quantum-classification/final_filtered_datasets/indicator_features.parquet',\n",
    "    \n",
    "    # Feature selection parameters\n",
    "    'n_features_stage1': 50000,    # Mutual Info coarse filter\n",
    "    'n_features_final': 500,       # XGBoost fine filter\n",
    "    \n",
    "    # Data modalities to process\n",
    "    'modalities': ['CNV', 'GeneExpr', 'miRNA', 'Meth', 'Prot', 'SNV'],\n",
    "    \n",
    "    # Memory management\n",
    "    'thrift_limit': 1 * 1024**3,   # 1GB for parquet loading\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "def safe_load_parquet(file_path, columns=None):\n",
    "    \"\"\"Load parquet file with increased thrift limit for large genomic data.\"\"\"\n",
    "    try:\n",
    "        return pd.read_parquet(\n",
    "            file_path,\n",
    "            columns=columns,\n",
    "            thrift_string_size_limit=CONFIG['thrift_limit'],\n",
    "            thrift_container_size_limit=CONFIG['thrift_limit']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# Create output directory\n",
    "ensure_dir(CONFIG['output_dir'])\n",
    "print(f\"Configuration loaded. Output: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 & 2: Two-Stage Feature Selection\n",
    "\n",
    "For each modality:\n",
    "1. **Load data** and filter to non-missing cases for robust feature selection\n",
    "2. **Mutual Information filter**: Reduce to top 50K features based on univariate MI scores\n",
    "3. **XGBoost filter**: Train classifier, select top 500 features by importance\n",
    "4. **Reload & save**: Reload only selected columns to minimize memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_two_stage_selection(modality, indicators_df, label_encoder, master_sort_order):\n",
    "    \"\"\"\n",
    "    Apply two-stage feature selection (MI -> XGBoost) to a single modality.\n",
    "    \n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(CONFIG['source_dir'], f'data_{modality}_.parquet')\n",
    "    output_file = os.path.join(CONFIG['output_dir'], f'data_{modality}_.parquet')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {modality}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load full dataset\n",
    "    df_full = safe_load_parquet(input_file)\n",
    "    if df_full is None:\n",
    "        return False\n",
    "    \n",
    "    n_features = df_full.shape[1] - 2  # Exclude case_id and class\n",
    "    print(f\"  Original features: {n_features:,}\")\n",
    "    \n",
    "    # Skip selection if already small enough\n",
    "    if n_features <= CONFIG['n_features_final']:\n",
    "        print(f\"  -> Dataset already small enough. Sorting and saving directly.\")\n",
    "        df_full.set_index('case_id', inplace=True)\n",
    "        df_sorted = df_full.reindex(master_sort_order).reset_index()\n",
    "        df_sorted.to_parquet(output_file, index=False)\n",
    "        return True\n",
    "    \n",
    "    # --- Filter to non-missing cases for robust feature selection ---\n",
    "    is_missing_col = f'is_missing_{modality}_'\n",
    "    case_ids_present = indicators_df[indicators_df[is_missing_col] == 0].index\n",
    "    df_for_selection = df_full[df_full['case_id'].isin(case_ids_present)]\n",
    "    del df_full\n",
    "    \n",
    "    X = df_for_selection.drop(columns=['case_id', 'class', is_missing_col], errors='ignore')\n",
    "    y = label_encoder.transform(df_for_selection['class'])\n",
    "    del df_for_selection\n",
    "    \n",
    "    print(f\"  Cases for selection: {len(y):,}\")\n",
    "    \n",
    "    # --- STAGE 1: Mutual Information Filter (coarse sieve) ---\n",
    "    print(f\"  STAGE 1: Mutual Information filter -> {CONFIG['n_features_stage1']:,} features\")\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    n_stage1 = min(CONFIG['n_features_stage1'], X.shape[1])\n",
    "    mi_selector = SelectKBest(mutual_info_classif, k=n_stage1)\n",
    "    mi_selector.fit(X_imputed, y)\n",
    "    del X_imputed\n",
    "    \n",
    "    stage1_features = X.columns[mi_selector.get_support()].tolist()\n",
    "    print(f\"    -> {len(stage1_features):,} features selected\")\n",
    "    del mi_selector\n",
    "    \n",
    "    # --- STAGE 2: XGBoost Importance Filter (fine comb) ---\n",
    "    print(f\"  STAGE 2: XGBoost filter -> {CONFIG['n_features_final']} features\")\n",
    "    \n",
    "    X_stage2 = X[stage1_features]\n",
    "    del X\n",
    "    \n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.1\n",
    "    )\n",
    "    \n",
    "    xgb_selector = SelectFromModel(\n",
    "        xgb_model,\n",
    "        max_features=CONFIG['n_features_final'],\n",
    "        prefit=False\n",
    "    ).fit(X_stage2.values, y)\n",
    "    del y\n",
    "    \n",
    "    final_features = X_stage2.columns[xgb_selector.get_support()].tolist()\n",
    "    print(f\"    -> {len(final_features)} features selected\")\n",
    "    del X_stage2\n",
    "    \n",
    "    # --- Reload only selected columns and save ---\n",
    "    print(f\"  Reloading selected columns and saving...\")\n",
    "    columns_to_load = ['case_id', 'class'] + final_features\n",
    "    df_final = safe_load_parquet(input_file, columns=columns_to_load)\n",
    "    \n",
    "    df_final.set_index('case_id', inplace=True)\n",
    "    df_sorted = df_final.reindex(master_sort_order).reset_index()\n",
    "    df_sorted.to_parquet(output_file, index=False)\n",
    "    \n",
    "    print(f\"  -> Saved: {os.path.basename(output_file)}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load indicators and create master sort order ---\n",
    "print(\"Loading indicator file and creating label encoder...\")\n",
    "\n",
    "try:\n",
    "    indicators_df = pd.read_parquet(CONFIG['indicator_file'])\n",
    "    master_sort_order = indicators_df['case_id'].sort_values().tolist()\n",
    "    \n",
    "    # Create label encoder for class labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(indicators_df['class'])\n",
    "    print(f\"  Classes: {list(label_encoder.classes_)}\")\n",
    "    print(f\"  Master sort order: {len(master_sort_order)} case_ids\")\n",
    "    \n",
    "    # Set index for efficient filtering\n",
    "    indicators_df.set_index('case_id', inplace=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    raise RuntimeError(f\"Indicator file not found: {CONFIG['indicator_file']}\")\n",
    "\n",
    "# --- Process all modalities ---\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting Two-Stage Funnel Feature Selection\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for modality in CONFIG['modalities']:\n",
    "    apply_two_stage_selection(modality, indicators_df, label_encoder, master_sort_order)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Feature selection complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Output Validation\n",
    "\n",
    "Scan all output parquet files to verify data quality (NaN presence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T10:03:06.319122Z",
     "iopub.status.busy": "2025-09-22T10:03:06.318018Z",
     "iopub.status.idle": "2025-09-22T10:04:06.886820Z",
     "shell.execute_reply": "2025-09-22T10:04:06.885974Z",
     "shell.execute_reply.started": "2025-09-22T10:03:06.319082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scan_directory_for_nans(directory):\n",
    "    \"\"\"Scan all parquet files in a directory for NaN values.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"  Warning: Directory not found: '{directory}'\")\n",
    "        return 0\n",
    "    \n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    if not files:\n",
    "        print(f\"  No Parquet files found in {directory}\")\n",
    "        return 0\n",
    "    \n",
    "    total_nans = 0\n",
    "    for filename in sorted(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = safe_load_parquet(file_path)\n",
    "            nan_count = df.isnull().sum().sum()\n",
    "            total_nans += nan_count\n",
    "            status = f\"{nan_count:,} NaN values\" if nan_count > 0 else \"Clean ✓\"\n",
    "            print(f\"    {filename}: {status}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {filename}: Error - {e}\")\n",
    "    \n",
    "    return total_nans\n",
    "\n",
    "# --- Scan directories ---\n",
    "dirs_to_scan = [\n",
    "    CONFIG['source_dir'],           # Input directory\n",
    "    CONFIG['output_dir'],           # Output directory\n",
    "]\n",
    "\n",
    "print(\"--- NaN Validation Scan ---\\n\")\n",
    "\n",
    "total_nans = 0\n",
    "for directory in dirs_to_scan:\n",
    "    print(f\"Scanning: {directory}\")\n",
    "    total_nans += scan_directory_for_nans(directory)\n",
    "    print()\n",
    "\n",
    "print(f\"Total NaN values found: {total_nans:,}\")\n",
    "if total_nans == 0:\n",
    "    print(\"✓ All data is clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Packaging\n",
    "\n",
    "Copy indicator features and create archive for distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T10:05:58.847004Z",
     "iopub.status.busy": "2025-09-22T10:05:58.846621Z",
     "iopub.status.idle": "2025-09-22T10:05:59.715122Z",
     "shell.execute_reply": "2025-09-22T10:05:59.713912Z",
     "shell.execute_reply.started": "2025-09-22T10:05:58.846963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy indicator features to output directory\n",
    "src_indicator = CONFIG['indicator_file']\n",
    "dst_indicator = os.path.join(CONFIG['output_dir'], 'indicator_features.parquet')\n",
    "\n",
    "if os.path.exists(src_indicator):\n",
    "    shutil.copy(src_indicator, dst_indicator)\n",
    "    print(f\"✓ Copied: indicator_features.parquet\")\n",
    "else:\n",
    "    print(f\"Warning: Indicator file not found at {src_indicator}\")\n",
    "\n",
    "# Create compressed archive\n",
    "archive_name = 'xgb_reduced.zip'\n",
    "print(f\"Creating archive: {archive_name}\")\n",
    "shutil.make_archive('xgb_reduced', 'zip', CONFIG['output_dir'])\n",
    "print(f\"✓ Archive created: {archive_name}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Pipeline Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Output directory: {CONFIG['output_dir']}/\")\n",
    "print(f\"  - 6 modality parquets @ {CONFIG['n_features_final']} features each\")\n",
    "print(f\"  - indicator_features.parquet\")\n",
    "print(f\"Archive: {archive_name}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8315143,
     "sourceId": 13126039,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 287017435,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
