{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline\n",
    "\n",
    "This notebook implements a 10-stage pipeline for processing multi-omics cancer data:\n",
    "\n",
    "1. **Setup**: Clone/update repository\n",
    "2. **Data Collection**: Create tumor-type TSVs from Kaggle datasets\n",
    "3. **Column Alignment**: Find common columns across tumor types\n",
    "4. **Merge**: Combine aligned TSVs into single dataset\n",
    "5. **Split by Modality**: Create separate parquet files per data type\n",
    "6. **Analysis**: Analyze NaN patterns and feature quality\n",
    "7. **Selective Imputation**: Impute only completely empty rows\n",
    "8. **Modality-Specific Processing**: Apply tailored strategies per data type\n",
    "9. **Indicator Generation**: Create missing modality flags\n",
    "10. **Case Selection**: Select balanced subset with minimal missingness\n",
    "\n",
    "## Configuration & Shared Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shared Configuration and Utility Functions\n",
    "All paths and constants are defined here for easy modification.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these paths for your environment\n",
    "# =============================================================================\n",
    "CONFIG = {\n",
    "    # Kaggle input paths\n",
    "    'KAGGLE_INPUT_BASE': '/kaggle/input/organized-gbm-lgg',\n",
    "    'WORKING_DIR': '/kaggle/working/quantum-classification',\n",
    "    \n",
    "    # Tumor types to process\n",
    "    'TUMOR_TYPES': ['astrocytoma', 'glioblastoma', 'mixed_glioma', 'oligodendroglioma'],\n",
    "    \n",
    "    # Feature prefixes (modality types)\n",
    "    'FEATURE_PREFIXES': ['Meth_', 'CNV_', 'GeneExpr_', 'miRNA_', 'SNV_', 'Prot_'],\n",
    "    \n",
    "    # Column names\n",
    "    'ID_COL': 'case_id',\n",
    "    'LABEL_COL': 'class',\n",
    "    \n",
    "    # Processing parameters\n",
    "    'CHUNK_SIZE': 100,\n",
    "    'NAN_THRESHOLD_DENSE': 90.0,\n",
    "    'NAN_THRESHOLD_SNV': 95.0,\n",
    "    'IMPUTE_VALUE': 0,\n",
    "    'TOP_N_CASES_PER_CLASS': 78,\n",
    "    \n",
    "    # Output directories (relative to WORKING_DIR)\n",
    "    'OUTPUTS_DIR': 'outputs',\n",
    "    'OUTPUTS_COMMON_DIR': 'outputs_common',\n",
    "    'FEATURE_SUBSETS_DIR': 'feature_subsets',\n",
    "    'FEATURE_SUBSETS_PROCESSED_DIR': 'feature_subsets_processed_selectively',\n",
    "    'FINAL_PROCESSED_DIR': 'final_processed_datasets',\n",
    "    'FINAL_FILTERED_DIR': 'final_filtered_datasets',\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# SHARED UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "def safe_load_parquet(file_path, columns=None):\n",
    "    \"\"\"\n",
    "    Safely loads a parquet file with increased thrift limits for large metadata.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the parquet file\n",
    "        columns: Optional list of columns to load (for memory efficiency)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or None if loading fails\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"  WARNING: File not found: {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        limit = 1 * 1024**3  # 1 GB limit\n",
    "        return pd.read_parquet(\n",
    "            file_path,\n",
    "            columns=columns,\n",
    "            thrift_string_size_limit=limit,\n",
    "            thrift_container_size_limit=limit\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_feature_cols(df, exclude_cols=None):\n",
    "    \"\"\"Get feature columns (excluding ID, label, and specified columns).\"\"\"\n",
    "    exclude = {CONFIG['ID_COL'], CONFIG['LABEL_COL']}\n",
    "    if exclude_cols:\n",
    "        exclude.update(exclude_cols)\n",
    "    return [col for col in df.columns if col not in exclude]\n",
    "\n",
    "\n",
    "print(\"âœ… Configuration and utilities loaded successfully\")\n",
    "print(f\"   Tumor types: {CONFIG['TUMOR_TYPES']}\")\n",
    "print(f\"   Feature prefixes: {CONFIG['FEATURE_PREFIXES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1-2: Repository Setup & Data Collection\n",
    "\n",
    "Clone the repository and create per-tumor-type TSV files from the Kaggle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T06:00:46.068041Z",
     "iopub.status.busy": "2025-09-18T06:00:46.067747Z",
     "iopub.status.idle": "2025-09-18T06:14:42.758660Z",
     "shell.execute_reply": "2025-09-18T06:14:42.757103Z",
     "shell.execute_reply.started": "2025-09-18T06:00:46.068017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Repository setup (run on Kaggle)\n",
    "# !rm -r /kaggle/working/quantum-classification\n",
    "# !git clone https://github.com/vimalathithan17/quantum-classification\n",
    "# %cd quantum-classification\n",
    "# !git pull\n",
    "\n",
    "# Create per-tumor-type TSV files\n",
    "for tumor_type in CONFIG['TUMOR_TYPES']:\n",
    "    input_dir = f\"{CONFIG['KAGGLE_INPUT_BASE']}/organizedTop10_{tumor_type}\"\n",
    "    output_file = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_DIR']}/{tumor_type}.tsv\"\n",
    "    print(f\"Creating TSV for {tumor_type}...\")\n",
    "    # Uncomment to run: !python3 py/create_multiomics.py --root {input_dir} --out {output_file} --label {tumor_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T07:08:51.886625Z",
     "iopub.status.busy": "2025-09-18T07:08:51.886241Z",
     "iopub.status.idle": "2025-09-18T07:08:51.904165Z",
     "shell.execute_reply": "2025-09-18T07:08:51.903494Z",
     "shell.execute_reply.started": "2025-09-18T07:08:51.886597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze missing files per tumor type (diagnostic)\n",
    "for tumor_type in CONFIG['TUMOR_TYPES']:\n",
    "    missing_file = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_DIR']}/{tumor_type}_missing_files.tsv\"\n",
    "    if os.path.exists(missing_file):\n",
    "        df = pd.read_csv(missing_file, delimiter='\\t')\n",
    "        print(f\"{tumor_type}: {df.sum().sum()} missing entries\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3-4: Column Alignment & Merge\n",
    "\n",
    "Find common columns across all tumor types and align features for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T09:31:48.123452Z",
     "iopub.status.busy": "2025-09-18T09:31:48.123096Z",
     "iopub.status.idle": "2025-09-18T10:30:42.972768Z",
     "shell.execute_reply": "2025-09-18T10:30:42.971164Z",
     "shell.execute_reply.started": "2025-09-18T09:31:48.123425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def align_columns_across_files(file_names, input_dir, output_dir, json_log_path):\n",
    "    \"\"\"\n",
    "    Find common columns across TSV files and save aligned versions.\n",
    "    \n",
    "    This solves the fundamental problem that different tumor types have \n",
    "    different feature sets - we need to find the intersection.\n",
    "    \"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "    columns_by_file = {}\n",
    "    \n",
    "    # First pass: read headers only (memory efficient)\n",
    "    print(\"--- Reading headers to identify column sets ---\")\n",
    "    for fname in file_names:\n",
    "        f_path = os.path.join(input_dir, f'{fname}.tsv')\n",
    "        try:\n",
    "            df_header = pd.read_csv(f_path, nrows=0, delimiter='\\t')\n",
    "            columns_by_file[fname] = set(df_header.columns)\n",
    "            print(f\"  {fname}: {len(df_header.columns)} columns\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  WARNING: {f_path} not found, skipping\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR reading {f_path}: {e}\")\n",
    "\n",
    "    if not columns_by_file:\n",
    "        print(\"No files read successfully. Exiting.\")\n",
    "        return []\n",
    "\n",
    "    # Find intersection\n",
    "    common_columns = sorted(set.intersection(*columns_by_file.values()))\n",
    "    print(f\"\\nâœ… Found {len(common_columns)} common columns\")\n",
    "    \n",
    "    # Log analysis\n",
    "    analysis = {\n",
    "        \"common_columns_count\": len(common_columns),\n",
    "        \"common_columns\": common_columns,\n",
    "        \"uncommon_by_file\": {\n",
    "            fname: sorted(cols - set(common_columns))\n",
    "            for fname, cols in columns_by_file.items()\n",
    "        }\n",
    "    }\n",
    "    with open(json_log_path, 'w') as f:\n",
    "        json.dump(analysis, f, indent=2)\n",
    "    print(f\"   Analysis saved to {json_log_path}\")\n",
    "\n",
    "    # Second pass: filter and save\n",
    "    print(\"\\n--- Saving aligned files ---\")\n",
    "    for fname in file_names:\n",
    "        input_path = os.path.join(input_dir, f'{fname}.tsv')\n",
    "        output_path = os.path.join(output_dir, f'{fname}.tsv')\n",
    "        try:\n",
    "            df = pd.read_csv(input_path, delimiter='\\t')\n",
    "            df[common_columns].to_csv(output_path, sep='\\t', index=False)\n",
    "            print(f\"  âœ… Saved {fname}.tsv\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {fname}: {e}\")\n",
    "    \n",
    "    return common_columns\n",
    "\n",
    "# Run alignment\n",
    "input_folder = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_DIR']}\"\n",
    "output_folder = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_COMMON_DIR']}\"\n",
    "json_file = f\"{CONFIG['WORKING_DIR']}/column_analysis.json\"\n",
    "\n",
    "common_cols = align_columns_across_files(\n",
    "    CONFIG['TUMOR_TYPES'], input_folder, output_folder, json_file\n",
    ")\n",
    "print(f\"\\nâœ… Column alignment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T10:46:12.124046Z",
     "iopub.status.busy": "2025-09-18T10:46:12.123683Z",
     "iopub.status.idle": "2025-09-18T10:46:30.014311Z",
     "shell.execute_reply": "2025-09-18T10:46:30.012924Z",
     "shell.execute_reply.started": "2025-09-18T10:46:12.124023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge all aligned files into single TSV\n",
    "output_common = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_COMMON_DIR']}\"\n",
    "merged_file = f\"{output_common}/merged.tsv\"\n",
    "\n",
    "# Shell command to merge (header from first file, data from all)\n",
    "tumor_files = \" \".join([f\"'{output_common}/{t}.tsv'\" for t in CONFIG['TUMOR_TYPES']])\n",
    "print(f\"Merging files into {merged_file}...\")\n",
    "# Run: !{{ head -n1 {output_common}/{CONFIG['TUMOR_TYPES'][0]}.tsv; for f in {tumor_files}; do tail -n+2 \"$f\"; done; }} > {merged_file}\n",
    "# !wc -l {merged_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T04:51:36.584663Z",
     "iopub.status.busy": "2025-09-19T04:51:36.584301Z",
     "iopub.status.idle": "2025-09-19T04:51:48.856920Z",
     "shell.execute_reply": "2025-09-19T04:51:48.855897Z",
     "shell.execute_reply.started": "2025-09-19T04:51:36.584636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze feature composition by prefix\n",
    "sample_file = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_DIR']}/{CONFIG['TUMOR_TYPES'][0]}.tsv\"\n",
    "df_header = pd.read_csv(sample_file, nrows=1, delimiter='\\t')\n",
    "\n",
    "prefix_counts = defaultdict(int)\n",
    "for col in df_header.columns:\n",
    "    prefix = col.split('_')[0]\n",
    "    prefix_counts[prefix] += 1\n",
    "\n",
    "print(\"Feature counts by modality prefix:\")\n",
    "for prefix, count in sorted(prefix_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {prefix}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Split by Modality (Memory-Efficient Streaming)\n",
    "\n",
    "Split the merged TSV into separate Parquet files per modality using chunked streaming to handle large files without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T05:39:13.790357Z",
     "iopub.status.busy": "2025-09-19T05:39:13.790073Z",
     "iopub.status.idle": "2025-09-19T06:30:51.097130Z",
     "shell.execute_reply": "2025-09-19T06:30:51.095765Z",
     "shell.execute_reply.started": "2025-09-19T05:39:13.790336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_by_modality_streaming(source_file, output_dir, prefixes, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Split a large TSV into modality-specific Parquet files using chunked streaming.\n",
    "    \n",
    "    This is memory-efficient: processes the file in chunks rather than loading all at once.\n",
    "    Uses PyArrow ParquetWriter for efficient incremental writes.\n",
    "    \"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "    writers = {}\n",
    "    \n",
    "    print(f\"Splitting {source_file} by modality...\")\n",
    "    chunk_iter = pd.read_csv(source_file, chunksize=chunk_size, delimiter='\\t')\n",
    "    \n",
    "    try:\n",
    "        for i, chunk in enumerate(chunk_iter):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Processing chunk {i + 1}...\")\n",
    "            \n",
    "            for prefix in prefixes:\n",
    "                base_cols = [CONFIG['ID_COL'], CONFIG['LABEL_COL']]\n",
    "                feature_cols = [c for c in chunk.columns if c.startswith(prefix)]\n",
    "                \n",
    "                if not feature_cols:\n",
    "                    continue\n",
    "                \n",
    "                subset = chunk[base_cols + feature_cols]\n",
    "                table = pa.Table.from_pandas(subset, preserve_index=False)\n",
    "                \n",
    "                # Initialize writer on first chunk\n",
    "                if i == 0 and prefix not in writers:\n",
    "                    output_file = os.path.join(output_dir, f'data_{prefix}.parquet')\n",
    "                    writers[prefix] = pq.ParquetWriter(output_file, table.schema)\n",
    "                \n",
    "                if prefix in writers:\n",
    "                    writers[prefix].write_table(table)\n",
    "    finally:\n",
    "        # Always close writers\n",
    "        for prefix, writer in writers.items():\n",
    "            writer.close()\n",
    "            print(f\"  âœ… Closed {prefix}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Created {len(writers)} modality files in {output_dir}\")\n",
    "\n",
    "# Run splitting\n",
    "source = f\"{CONFIG['WORKING_DIR']}/{CONFIG['OUTPUTS_COMMON_DIR']}/merged.tsv\"\n",
    "output = CONFIG['FEATURE_SUBSETS_DIR']\n",
    "split_by_modality_streaming(source, output, CONFIG['FEATURE_PREFIXES'], CONFIG['CHUNK_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Feature Quality Analysis\n",
    "\n",
    "Analyze NaN patterns to understand data quality and guide preprocessing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T04:15:50.756474Z",
     "iopub.status.busy": "2025-09-20T04:15:50.756155Z",
     "iopub.status.idle": "2025-09-20T04:30:08.115927Z",
     "shell.execute_reply": "2025-09-20T04:30:08.114555Z",
     "shell.execute_reply.started": "2025-09-20T04:15:50.756448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_feature_quality(data_dir, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze NaN patterns in each modality file.\n",
    "    \n",
    "    Reports:\n",
    "    - Overall NaN percentage\n",
    "    - Completely empty rows (missing entire modality)\n",
    "    - Sporadic missing values (within rows that have some data)\n",
    "    \"\"\"\n",
    "    print(f\"--- Feature Quality Analysis: {data_dir} ---\\n\")\n",
    "    \n",
    "    for filename in sorted(os.listdir(data_dir)):\n",
    "        if not filename.endswith('.parquet'):\n",
    "            continue\n",
    "            \n",
    "        print(f\"ðŸ“Š {filename}\")\n",
    "        df = safe_load_parquet(os.path.join(data_dir, filename))\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        rows, cols = df.shape\n",
    "        feature_cols = get_feature_cols(df)\n",
    "        \n",
    "        # Overall NaN stats\n",
    "        total_nan = df[feature_cols].isnull().sum().sum()\n",
    "        total_cells = len(feature_cols) * rows\n",
    "        nan_pct = (total_nan / total_cells) * 100 if total_cells > 0 else 0\n",
    "        \n",
    "        # Empty row analysis\n",
    "        empty_mask = df[feature_cols].isnull().all(axis=1)\n",
    "        n_empty = empty_mask.sum()\n",
    "        \n",
    "        # Sporadic NaN analysis (after removing empty rows)\n",
    "        df_nonempty = df[~empty_mask]\n",
    "        sporadic_nan = df_nonempty[feature_cols].isnull().sum().sum() if len(df_nonempty) > 0 else 0\n",
    "        \n",
    "        print(f\"   Shape: {rows:,} rows Ã— {cols:,} cols\")\n",
    "        print(f\"   NaN: {nan_pct:.1f}% ({total_nan:,} cells)\")\n",
    "        print(f\"   Empty rows: {n_empty} ({n_empty/rows*100:.1f}%)\")\n",
    "        print(f\"   Sporadic NaN: {sporadic_nan:,}\")\n",
    "        \n",
    "        # Top missing features\n",
    "        if total_nan > 0:\n",
    "            top_missing = df[feature_cols].isnull().sum().nlargest(top_n)\n",
    "            print(f\"   Top {top_n} missing features:\")\n",
    "            for feat, count in top_missing.items():\n",
    "                print(f\"      {feat[:40]}: {count}\")\n",
    "        print()\n",
    "\n",
    "analyze_feature_quality(CONFIG['FEATURE_SUBSETS_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Selective Imputation & Indicator Creation\n",
    "\n",
    "Create binary indicators for missing modalities and selectively impute only completely empty rows (preserving sporadic NaNs for models that can learn from missingness patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T05:06:40.391870Z",
     "iopub.status.busy": "2025-09-20T05:06:40.390981Z",
     "iopub.status.idle": "2025-09-20T05:46:01.720009Z",
     "shell.execute_reply": "2025-09-20T05:46:01.718582Z",
     "shell.execute_reply.started": "2025-09-20T05:06:40.391831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def selective_imputation(source_dir, output_dir, impute_value=0):\n",
    "    \"\"\"\n",
    "    Create indicator columns and selectively impute only completely empty rows.\n",
    "    \n",
    "    Key insight: We preserve sporadic NaNs because models like LightGBM/XGBoost\n",
    "    can learn from missingness patterns. Only completely empty rows (missing \n",
    "    entire modality) are filled with placeholder values.\n",
    "    \"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "    print(f\"--- Selective Imputation: {source_dir} â†’ {output_dir} ---\\n\")\n",
    "    \n",
    "    for filename in sorted(os.listdir(source_dir)):\n",
    "        if not filename.endswith('.parquet'):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {filename}...\")\n",
    "        df = safe_load_parquet(os.path.join(source_dir, filename))\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        feature_cols = get_feature_cols(df)\n",
    "        \n",
    "        # Identify completely empty rows\n",
    "        empty_mask = df[feature_cols].isnull().all(axis=1)\n",
    "        \n",
    "        # Create indicator column\n",
    "        prefix = filename.replace('data_', '').replace('.parquet', '')\n",
    "        indicator_col = f\"is_missing_{prefix}\"\n",
    "        df[indicator_col] = empty_mask.astype(int)\n",
    "        \n",
    "        # Impute ONLY empty rows\n",
    "        df.loc[empty_mask, feature_cols] = impute_value\n",
    "        \n",
    "        # Save\n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        df.to_parquet(output_file, index=False)\n",
    "        \n",
    "        n_imputed = empty_mask.sum()\n",
    "        print(f\"  âœ… Added '{indicator_col}', imputed {n_imputed} empty rows\")\n",
    "    \n",
    "    print(f\"\\nâœ… Selective imputation complete\")\n",
    "\n",
    "selective_imputation(\n",
    "    CONFIG['FEATURE_SUBSETS_DIR'],\n",
    "    CONFIG['FEATURE_SUBSETS_PROCESSED_DIR'],\n",
    "    CONFIG['IMPUTE_VALUE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: NaN Distribution Analysis\n",
    "\n",
    "Verify the selective imputation worked correctly by analyzing remaining NaN patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T07:08:40.725658Z",
     "iopub.status.busy": "2025-09-20T07:08:40.725314Z",
     "iopub.status.idle": "2025-09-20T07:09:34.548214Z",
     "shell.execute_reply": "2025-09-20T07:09:34.547151Z",
     "shell.execute_reply.started": "2025-09-20T07:08:40.725633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_nan_distribution(data_dir, bins=None):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of NaN percentages across columns.\n",
    "    \n",
    "    This helps verify selective imputation worked and understand\n",
    "    what proportion of columns have different levels of missingness.\n",
    "    \"\"\"\n",
    "    if bins is None:\n",
    "        bins = [0, 10, 30, 50, 70, 90, 95, 97, 100]\n",
    "    \n",
    "    print(f\"--- NaN Distribution Analysis: {data_dir} ---\\n\")\n",
    "    \n",
    "    for filename in sorted(os.listdir(data_dir)):\n",
    "        if not filename.endswith('.parquet'):\n",
    "            continue\n",
    "            \n",
    "        print(f\"ðŸ“Š {filename}\")\n",
    "        df = safe_load_parquet(os.path.join(data_dir, filename))\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        total_cols = len(df.columns)\n",
    "        nan_pct = df.isnull().mean() * 100\n",
    "        \n",
    "        # Count columns with 0 NaN\n",
    "        no_nan = (nan_pct == 0).sum()\n",
    "        \n",
    "        # Bin columns with NaN\n",
    "        labels = [f'{bins[i]}-{bins[i+1]}%' for i in range(len(bins)-1)]\n",
    "        binned = pd.cut(nan_pct[nan_pct > 0], bins=bins, labels=labels, include_lowest=True)\n",
    "        dist = binned.value_counts().sort_index()\n",
    "        \n",
    "        print(f\"   {'0%':<12}: {no_nan:>6} cols ({no_nan/total_cols*100:.1f}%)\")\n",
    "        for label, count in dist.items():\n",
    "            pct = count / total_cols * 100\n",
    "            print(f\"   {label:<12}: {count:>6} cols ({pct:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "analyze_nan_distribution(CONFIG['FEATURE_SUBSETS_PROCESSED_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 9: Modality-Specific Processing\n",
    "\n",
    "Apply tailored preprocessing strategies based on each modality's characteristics:\n",
    "- **Dense data** (miRNA, GeneExpr, CNV, Prot): Drop columns with >90% NaN\n",
    "- **Methylation**: Preserve NaN patterns (models can learn from missingness)\n",
    "- **SNV**: Handle sparse mutation data appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T07:13:49.271893Z",
     "iopub.status.busy": "2025-09-20T07:13:49.271574Z",
     "iopub.status.idle": "2025-09-20T07:16:30.623008Z",
     "shell.execute_reply": "2025-09-20T07:16:30.621769Z",
     "shell.execute_reply.started": "2025-09-20T07:13:49.271863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modality-specific processing strategies\n",
    "\n",
    "def process_dense_data(df, nan_threshold=90.0):\n",
    "    \"\"\"Process dense, high-quality data by removing nearly-empty columns.\"\"\"\n",
    "    feature_cols = get_feature_cols(df)\n",
    "    nan_pct = df[feature_cols].isnull().mean() * 100\n",
    "    cols_to_drop = nan_pct[nan_pct > nan_threshold].index.tolist()\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        print(f\"    Dropped {len(cols_to_drop)} columns with >{nan_threshold}% NaN\")\n",
    "    return df\n",
    "\n",
    "def process_meth_data(df):\n",
    "    \"\"\"Preserve methylation data with NaN patterns intact.\"\"\"\n",
    "    print(\"    Preserved NaN patterns for LightGBM/XGBoost\")\n",
    "    return df\n",
    "\n",
    "def process_snv_data(df):\n",
    "    \"\"\"Handle sparse SNV mutation data.\"\"\"\n",
    "    print(\"    Preserved sparse mutation patterns\")\n",
    "    return df\n",
    "\n",
    "def apply_modality_processing(source_dir, output_dir):\n",
    "    \"\"\"Apply appropriate strategy to each modality file.\"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "    \n",
    "    strategies = {\n",
    "        'miRNA_': ('dense', process_dense_data),\n",
    "        'GeneExpr_': ('dense', process_dense_data),\n",
    "        'CNV_': ('dense', process_dense_data),\n",
    "        'Prot_': ('dense', process_dense_data),\n",
    "        'Meth_': ('preserve', process_meth_data),\n",
    "        'SNV_': ('sparse', process_snv_data),\n",
    "    }\n",
    "    \n",
    "    print(f\"--- Modality-Specific Processing ---\\n\")\n",
    "    \n",
    "    for filename in sorted(os.listdir(source_dir)):\n",
    "        if not filename.endswith('.parquet'):\n",
    "            continue\n",
    "            \n",
    "        # Find matching strategy\n",
    "        strategy_name, processor = None, None\n",
    "        for prefix, (name, func) in strategies.items():\n",
    "            if prefix in filename:\n",
    "                strategy_name, processor = name, func\n",
    "                break\n",
    "        \n",
    "        if processor is None:\n",
    "            print(f\"  âš ï¸ No strategy for {filename}, copying as-is\")\n",
    "            processor = lambda df: df\n",
    "            strategy_name = \"copy\"\n",
    "        \n",
    "        print(f\"  ðŸ“ {filename} [{strategy_name}]\")\n",
    "        df = safe_load_parquet(os.path.join(source_dir, filename))\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        df = processor(df)\n",
    "        df.to_parquet(os.path.join(output_dir, filename), index=False)\n",
    "        print(f\"    âœ… Saved ({df.shape[0]} rows, {df.shape[1]} cols)\")\n",
    "    \n",
    "    print(f\"\\nâœ… Modality processing complete â†’ {output_dir}\")\n",
    "\n",
    "apply_modality_processing(\n",
    "    CONFIG['FEATURE_SUBSETS_PROCESSED_DIR'],\n",
    "    CONFIG['FINAL_PROCESSED_DIR']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 10: Indicator Summary & Case Selection\n",
    "\n",
    "Generate a master indicator file summarizing missing modalities per case, then select the top cases per class with minimal missingness for a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T08:36:26.534021Z",
     "iopub.status.busy": "2025-09-21T08:36:26.533103Z",
     "iopub.status.idle": "2025-09-21T08:37:15.846438Z",
     "shell.execute_reply": "2025-09-21T08:37:15.845567Z",
     "shell.execute_reply.started": "2025-09-21T08:36:26.533989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_indicator_summary(source_dir):\n",
    "    \"\"\"\n",
    "    Create a master indicator file combining all is_missing_* columns.\n",
    "    \n",
    "    This enables downstream models to condition on data availability.\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(source_dir, 'indicator_features.parquet')\n",
    "    print(f\"--- Generating Indicator Summary ---\\n\")\n",
    "    \n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith('.parquet')]\n",
    "    if not files:\n",
    "        print(\"No parquet files found\")\n",
    "        return None\n",
    "    \n",
    "    # Get base DataFrame with case_id and class\n",
    "    first_file = os.path.join(source_dir, files[0])\n",
    "    base_df = safe_load_parquet(first_file)\n",
    "    if base_df is None:\n",
    "        return None\n",
    "    \n",
    "    dfs_to_merge = [base_df[[CONFIG['ID_COL'], CONFIG['LABEL_COL']]]]\n",
    "    \n",
    "    # Extract indicator columns from each file\n",
    "    for filename in files:\n",
    "        df = safe_load_parquet(os.path.join(source_dir, filename))\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        indicator_cols = [c for c in df.columns if c.startswith('is_missing_')]\n",
    "        if indicator_cols:\n",
    "            dfs_to_merge.append(df[[CONFIG['ID_COL']] + indicator_cols])\n",
    "            print(f\"  Extracted {indicator_cols} from {filename}\")\n",
    "    \n",
    "    # Merge all indicators\n",
    "    if len(dfs_to_merge) > 1:\n",
    "        final_df = reduce(\n",
    "            lambda left, right: pd.merge(left, right, on=CONFIG['ID_COL'], how='outer'),\n",
    "            dfs_to_merge\n",
    "        )\n",
    "        \n",
    "        # Fill any missing indicators with 0\n",
    "        indicator_cols = [c for c in final_df.columns if c.startswith('is_missing_')]\n",
    "        final_df[indicator_cols] = final_df[indicator_cols].fillna(0).astype(int)\n",
    "        \n",
    "        final_df.to_parquet(output_file, index=False)\n",
    "        print(f\"\\nâœ… Saved indicator summary: {output_file}\")\n",
    "        print(f\"   Shape: {final_df.shape}\")\n",
    "        print(final_df.head())\n",
    "        return final_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "indicator_df = generate_indicator_summary(CONFIG['FINAL_PROCESSED_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def select_balanced_cases(source_dir, output_dir, indicator_file, top_n=78):\n",
    "    \"\"\"\n",
    "    Select top N cases per class with minimal missingness for balanced training.\n",
    "    \n",
    "    This creates a balanced dataset where each class has equal representation\n",
    "    and cases are selected based on data completeness (fewer missing modalities).\n",
    "    \"\"\"\n",
    "    ensure_dir(output_dir)\n",
    "    print(f\"--- Selecting Top {top_n} Cases Per Class ---\\n\")\n",
    "    \n",
    "    # Load indicator file\n",
    "    indicator_df = safe_load_parquet(indicator_file)\n",
    "    if indicator_df is None:\n",
    "        print(\"Indicator file not found\")\n",
    "        return\n",
    "    \n",
    "    # Calculate missingness score\n",
    "    indicator_cols = [c for c in indicator_df.columns if c.startswith('is_missing_')]\n",
    "    indicator_df['missing_score'] = indicator_df[indicator_cols].sum(axis=1)\n",
    "    \n",
    "    # Select top N per class (lowest missing_score)\n",
    "    selected = (\n",
    "        indicator_df\n",
    "        .sort_values([CONFIG['LABEL_COL'], 'missing_score'], ascending=[True, True])\n",
    "        .groupby(CONFIG['LABEL_COL'])\n",
    "        .head(top_n)\n",
    "    )\n",
    "    \n",
    "    keep_ids = set(selected[CONFIG['ID_COL']].unique())\n",
    "    print(f\"Selected {len(keep_ids)} cases total\")\n",
    "    print(\"\\nCases per class:\")\n",
    "    print(selected[CONFIG['LABEL_COL']].value_counts())\n",
    "    \n",
    "    # Filter all modality files\n",
    "    print(\"\\n--- Filtering Files ---\")\n",
    "    files = [f for f in os.listdir(source_dir) \n",
    "             if f.endswith('.parquet') and f != 'indicator_features.parquet']\n",
    "    \n",
    "    for filename in files:\n",
    "        df = safe_load_parquet(os.path.join(source_dir, filename))\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        filtered = df[df[CONFIG['ID_COL']].isin(keep_ids)]\n",
    "        filtered.to_parquet(os.path.join(output_dir, filename), index=False)\n",
    "        print(f\"  âœ… {filename}: {len(filtered)} rows\")\n",
    "    \n",
    "    # Save filtered indicators\n",
    "    selected.to_parquet(os.path.join(output_dir, 'indicator_features.parquet'), index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Balanced dataset saved to {output_dir}\")\n",
    "\n",
    "indicator_file = os.path.join(CONFIG['FINAL_PROCESSED_DIR'], 'indicator_features.parquet')\n",
    "select_balanced_cases(\n",
    "    CONFIG['FINAL_PROCESSED_DIR'],\n",
    "    CONFIG['FINAL_FILTERED_DIR'],\n",
    "    indicator_file,\n",
    "    CONFIG['TOP_N_CASES_PER_CLASS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T08:38:29.344916Z",
     "iopub.status.busy": "2025-09-21T08:38:29.344149Z",
     "iopub.status.idle": "2025-09-21T08:39:57.129208Z",
     "shell.execute_reply": "2025-09-21T08:39:57.128120Z",
     "shell.execute_reply.started": "2025-09-21T08:38:29.344888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Package final output\n",
    "print(\"--- Packaging Output ---\")\n",
    "print(f\"!zip -r final_dataset.zip {CONFIG['FINAL_FILTERED_DIR']}\")\n",
    "# Uncomment to run: !zip -r final_dataset.zip {CONFIG['FINAL_FILTERED_DIR']}"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8284007,
     "sourceId": 13079644,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
